{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 심층 신경망 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_input, train_target), (test_input, test_target) = \\\n",
    "    keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_scaled = train_input / 255\n",
    "train_scaled = train_scaled.reshape(-1, 28 * 28)\n",
    "\n",
    "train_scaled, val_scaled, train_target, val_target = train_test_split( \\\n",
    "    train_scaled, train_target, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 은닉층 ( 층 추가 )\n",
    "입력층과 출력층 사이에 밀집층이 추가되었다  \n",
    "출력층에 적용하는 활성화 함수는 종류가 제한되어있다 ( 이진분류 : 시그모이드, 다중 분류 : 소프트맥스 ).  \n",
    "은닉층 활성화 함수는 비교적 자유롭다. 대표적으로 시그모이드 함수와 렐루 함수 등을 사용한다.  \n",
    "선형 계산을 적당하게 비선형적으로 비털어 주저야 다음 층의 계산과 단순히 합쳐지지 않는다.\n",
    "\n",
    "---\n",
    "회귀 출력은 활성화 함수를 적용ㅇ하지 않는다. 출력층의 선형 방정식의 계산 그대로를 출력한다! <br>\n",
    "  이를 위해서는 `Dense(activation = None)` 으로 지정한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 생성 방법 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100개의 뉴런을 가진 밀집층이며, 활성화 함수 시그모이드, 입력 크기 (784, )\n",
    "# 은닉층 뉴런 개수는 특별한 기준이 없다 > 개인 역량 \n",
    "# 그럼에도 불구하고...? 적어도 출력층의 뉴런보다는 많게 만들어야 한다.\n",
    "dense1 = keras.layers.Dense(100, activation = 'sigmoid', input_shape=(784, ), name = 'first')\n",
    "dense2 = keras.layers.Dense(10, activation = 'softmax', name = 'second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트틑 가장 처음 등장하는 은닉층에서 마지막 출력층의 순서로 나열해야 한다.\n",
    "model = keras.Sequential([dense1, dense2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.summary()\n",
    "\n",
    "\n",
    "Model: 모델의 이름\n",
    "\n",
    "---\n",
    "\n",
    "층 이름 (클래스)                  출력 크기                  모델 파라미터 개수\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "\n",
    "=================================================================\n",
    "\n",
    "first (Dense)                (None, 100)                78500 = 784개 * 100개 + 100개     \n",
    "    (100 > 샘플마다 784개의 픽셀값이 은닉층을 통과하면서 100개의 특성으로 압축됨)\n",
    "_________________________________________________________________\n",
    "\n",
    "second (Dense)               (None, 10)                1010 = 100개 * 10개 + 10개      \n",
    " 100개의 은닉층 뉴런과 1-개의 출력층 뉴런이 모두 연결되고, 출력층의 뉴련마다 하나의 절편이 있음\n",
    " \n",
    "=================================================================\n",
    "\n",
    "Total params: 79,510 은닉층과 출력층의 파라미터 개수를 합친 값\n",
    "Trainable params: 79,510\n",
    "Non-trainable params: 0 훈련되지 않은 파라미터 \n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "first (Dense)                (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "second (Dense)               (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 생성 방법 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\\\n",
    "                         keras.layers.Dense(100, activation = 'sigmoid', input_shape=(784, ), name = 'hidden'), \n",
    "                         keras.layers.Dense(10, activation = 'softmax', name = 'output'), ],\n",
    "                        name = '패션 MNIST 모델')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"패션 MNIST 모델\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden (Dense)               (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 생성 방법 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(100, activation = 'sigmoid', input_shape=(784, ), name = 'hidden'))\n",
    "model.add(keras.layers.Dense(10, activation = 'softmax', name = 'output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden (Dense)               (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 1s 643us/step - loss: 0.7616 - accuracy: 0.7541\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 1s 637us/step - loss: 0.4149 - accuracy: 0.8509\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 1s 622us/step - loss: 0.3740 - accuracy: 0.8645\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 1s 607us/step - loss: 0.3529 - accuracy: 0.8718\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 1s 603us/step - loss: 0.3344 - accuracy: 0.8775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f803c3dc2b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_scaled, train_target, epochs= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 371us/step - loss: 0.3473 - accuracy: 0.8744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3473353683948517, 0.8744166493415833]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_scaled, val_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU 함수\n",
    "이미지 분류 문제에서 높은 성능을 낼 수 있다.   \n",
    "입력이 양수일 경우 마치 활성화 함수가 없는 것처럼 입력을 통과시키고, 음수일 경우는 0으로 변환함.    \n",
    "`max(0, z)` : z가 0보다 크면 z를 출력하고, 0보다 작으면 0을 출력한다.\n",
    "<br><br>\n",
    "## Flatten 층\n",
    "numpy의 reshape을 대신해 사용할 수 있다.  \n",
    "flatten 클래스는 배치 차원을 제외하고 나머지 입력 차원을 모두 일렬로 펼치는 역할을 한다.   \n",
    "입력에 곱해지는 가중치나 절편이 없다.\n",
    "성능에 기여하지는 않지만, 입력층과 은닉층 사이에 추가하기 때문에 층이라 부른다!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = (28, 28)))\n",
    "model.add(keras.layers.Dense(100, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_input, train_target), (test_input, test_target) = \\\n",
    "    keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_scaled = train_input / 255\n",
    "# train_scaled = train_scaled.reshape(-1, 28 * 28)\n",
    "\n",
    "train_scaled, val_scaled, train_target, val_target = train_test_split( \\\n",
    "    train_scaled, train_target, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 1s 609us/step - loss: 0.6808 - accuracy: 0.7622\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 1s 603us/step - loss: 0.4019 - accuracy: 0.8539\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 1s 598us/step - loss: 0.3532 - accuracy: 0.8719\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 1s 590us/step - loss: 0.3298 - accuracy: 0.8808\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 1s 584us/step - loss: 0.3233 - accuracy: 0.8838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f803438f610>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', metrics = 'accuracy')\n",
    "model.fit(train_scaled, train_target, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 옵티마이저\n",
    "다양한 종류의 경사 하강법 알고리즘  \n",
    "`compile()` 메서드에서는 케라스 기본 경사 하강법 알고리즘인 RMSprop을 사용한다 (RMSprop 학습률 또한 조정할 하이퍼파라미터).   \n",
    "\n",
    "## Adagrad \n",
    "learning_rate : 학습률 지정 ( 기본 0.001 )  \n",
    "Adagrad는 그레디언트 제곱을 누적해 학습률을 나눈다.   \n",
    "initial_accumulator_value 매개변수에서 누적 초깃값을 지정할 수 있다 ( 0.1 )  \n",
    "\n",
    "## RMSprop\n",
    "learning_rate : 학습률 지정 ( 기본 0.001 )  \n",
    "Adagrad처럼 그레디언트 제곱으로 학습률을 나누지만, 최신 그레디언트를 사용하기 위해 지수 감소를 사용한다.  \n",
    "rho 매개변수에서 감소 비율을 지정한다. ( 0.9 )   \n",
    "\n",
    "## Adam\n",
    "learning_rate : 학습률 지정 ( 기본 0.001 )   \n",
    "모멘텀 최적화에 있는 그레디언트 지수 감소 평균을 조절하기 위해 beta_1 매개변수가 있다. ( 0.9 )  \n",
    "RMSprop에 있는 그레디언트 제곱의 지수 감소 평균을 조절하기 위해 beta_2 매개변수가 있다. ( 0.999 )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 학습률 값 0.01 \n",
    "# momentum 0보다 큰 값을 지정해, 이전 그레디언트를 가속도처럼 사용 ( 보통 0.9 이상 지정 )\n",
    "# nesterov 네스테로프 모멘텀 최적화\n",
    "\n",
    "sgd = keras.optimizers.SGD(learning_rate= 0.1, momentum = 0.9, nesterov = True)\n",
    "model.compile(optimizer = sgd, loss='sparse_categorical_crossentropy', metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  적응적 학습률 Adaptive Learning Rate\n",
    "모델이 최적점에 가까이 갈수록 학습률을 낮추면 안정적으로 최적점에 수렴할 가능성이 높다  \n",
    "대표적인 옵티마이저\n",
    "1. adagrad\n",
    "2. rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad = keras.optimizers.Adagrad()\n",
    "model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop()\n",
    "model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "모멘컴 최적화와 RMSprop의 장접을 접목시킨 옵티마이저."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "model.add(keras.layers.Dense(100, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 1s 459us/step - loss: 0.6858 - accuracy: 0.7652\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 1s 457us/step - loss: 0.4023 - accuracy: 0.8582\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 1s 457us/step - loss: 0.3573 - accuracy: 0.8730\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 1s 459us/step - loss: 0.3222 - accuracy: 0.8838\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 1s 458us/step - loss: 0.3035 - accuracy: 0.8905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f80416c5580>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics = 'accuracy')\n",
    "model.fit(train_scaled, train_target, epochs = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 368us/step - loss: 0.3363 - accuracy: 0.8781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.336306631565094, 0.878083348274231]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_scaled, val_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da",
   "language": "python",
   "name": "da"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
